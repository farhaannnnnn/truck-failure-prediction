{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16333b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf920b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/aps_failure_training_set.csv', na_values='na')\n",
    "    test_df = pd.read_csv('data/aps_failure_test_set.csv', na_values='na')\n",
    "    print(\"--- Datasets loaded successfully ---\")\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n---! ERROR !---\")\n",
    "    print(\"Dataset files not found. Please ensure your folder structure is correct.\")\n",
    "    print(\"The script expects a 'data' folder in the same directory, containing the CSV files.\")\n",
    "    # Exit gracefully if files are not found\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee46be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([train_df, test_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c5e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map target variable 'class' to numerical values\n",
    "class_mapping = {'neg': 0, 'pos': 1}\n",
    "combined_df['class'] = combined_df['class'].map(class_mapping)\n",
    "\n",
    "print(\"\\n--- Target variable 'class' mapped to 0s and 1s ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a6061",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_df.drop('class', axis=1)\n",
    "y = combined_df['class']\n",
    "\n",
    "# Check for class imbalance\n",
    "print(\"\\n--- Initial Class Distribution ---\")\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a78a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Convert back to a DataFrame to keep column names\n",
    "X_imputed_df = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "print(\"\\n--- Missing values handled using Median Imputation ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d3d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train_df)\n",
    "X_train_orig = X_imputed_df.iloc[:train_len]\n",
    "y_train_orig = y.iloc[:train_len]\n",
    "X_test = X_imputed_df.iloc[train_len:]\n",
    "y_test = y.iloc[train_len:]\n",
    "\n",
    "# --- Apply SMOTE only to the training data ---\n",
    "print(\"\\n--- Applying SMOTE to the training data to handle class imbalance... ---\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_orig, y_train_orig)\n",
    "\n",
    "print(\"--- SMOTE applied successfully ---\")\n",
    "print(\"\\n--- Class Distribution After SMOTE ---\")\n",
    "print(pd.Series(y_train_smote).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee333cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features after applying SMOTE to prevent data leakage\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_smote)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77870eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"\\n--- Training and evaluating models... ---\")\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train_smote)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    results[name] = f1\n",
    "    \n",
    "    print(f\"\\n--- Model: {name} ---\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Store and display confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    # You can save this figure to your 'images/' folder\n",
    "    # plt.savefig(f'images/confusion_matrix_{name.replace(\" \", \"_\")}.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ed599",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Final Model Comparison (based on F1-Score) ---\")\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index', columns=['F1-Score']).sort_values(by='F1-Score', ascending=False)\n",
    "print(results_df)\n",
    "\n",
    "best_model_name = results_df.index[0]\n",
    "best_model_score = results_df.iloc[0, 0]\n",
    "print(f\"\\nBest performing model is '{best_model_name}' with an F1-Score of {best_model_score:.4f}.\")\n",
    "print(\"\\n--- Project 1 script finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f09aa14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208217a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a8fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6430beb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
